{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538cf082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "3ce61287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 128, kernel_size=(12,3), padding=(0,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=(1,3), padding=(0,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 32, kernel_size=(1,3), padding=(0,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 16, kernel_size=(1,3), padding=(0,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 6, kernel_size=(1,3), padding=(0,1))\n",
    "        )\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.cnn(X)\n",
    "        X = self.flatten(X)\n",
    "        return X\n",
    "        \n",
    "class Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(6, 16, kernel_size=(1,3), padding=(0,1), output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(16, 32, kernel_size=(1,3), padding=(0,1), output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(32, 64, kernel_size=(1,3), padding=(0,1), output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(64, 128, kernel_size=(1,3), padding=(0,1), output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(128, 1, kernel_size=(12,3), padding=(0,1), output_padding=(0,0))\n",
    "        )\n",
    "        \n",
    "        self.unflatten = torch.nn.Unflatten(1, (6, 1, 5000))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.unflatten(X)\n",
    "        X = self.cnn(X)\n",
    "        return X\n",
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.linear = torch.nn.Linear(30000, 56)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.enc(X)\n",
    "        Y = self.linear(Y)\n",
    "        Y = torch.sigmoid(Y)\n",
    "        return Y\n",
    "    \n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.enc.forward(X)\n",
    "        X = self.dec.forward(X)\n",
    "        return X\n",
    "    \n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim=10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dimension of latent variable.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder: takes a sample and reduces down to a 30,000-dim vector.\n",
    "        self.enc = Encoder()\n",
    "        \n",
    "        # Layers for predicting parameters of q(z|x).\n",
    "        self.latent_mu = torch.nn.Linear(30000, hidden_dim)\n",
    "        self.latent_log_var = torch.nn.Linear(30000, hidden_dim)\n",
    "        \n",
    "        # Linear layer to upscale input to decoder.\n",
    "        self.dec_lin = torch.nn.Linear(hidden_dim, 30000)\n",
    "        \n",
    "        # Decoder: takes a 30,000-dim vector and upsamples to (12, 5000).\n",
    "        self.dec = Decoder()\n",
    "        \n",
    "    def sample_gaussian(self, mu, log_var):\n",
    "        B_SZ = mu.shape[0]\n",
    "        eps = torch.randn((B_SZ, self.hidden_dim))\n",
    "        return mu + eps * torch.exp(log_var / 2.0) # multiply by std. dev.\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Pass input through the Encoder.\n",
    "        enc_out = self.enc.forward(X)\n",
    "        \n",
    "        # Predict parameters of q(z|x).\n",
    "        z_mu = self.latent_mu(enc_out)\n",
    "        z_log_var = self.latent_log_var(enc_out)\n",
    "        \n",
    "        # Reparameterization \"trick\". \n",
    "        # Get a multivariate gaussian with means z_mu and variances z_var * I.\n",
    "        z = self.sample_gaussian(z_mu, z_log_var)\n",
    "        \n",
    "        # Pass latent sample through decoder.\n",
    "        dec_out = self.dec.forward(self.dec_lin(z))\n",
    "        \n",
    "        return dec_out, z_mu, z_log_var\n",
    "    \n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn((n, self.hidden_dim))\n",
    "            dec_out = self.dec.forward(self.dec_lin(z))\n",
    "            return dec_out \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "49ff7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(n = -1):\n",
    "    # Load Diagnostics.xlsx\n",
    "    diagnostics = pd.read_excel(\"data/Diagnostics.xlsx\")\n",
    "    \n",
    "    # Load Conditions.xlsx\n",
    "    conditions = pd.read_excel(\"data/ConditionNames.xlsx\")\n",
    "    n_cond = conditions[\"Acronym Name\"].size\n",
    "    cond_map = { conditions[\"Acronym Name\"][i] : i for i in range(n_cond) }\n",
    "    \n",
    "    # Load Examples from files.\n",
    "    num_examples = diagnostics.shape[0] if (n == -1) else n\n",
    "    \n",
    "    X = np.zeros((5000, num_examples, 12))\n",
    "    Y = np.zeros((num_examples, n_cond))\n",
    "\n",
    "    print(\"Loading\", num_examples, \"Examples.\")\n",
    "    for i, file in tqdm(enumerate((Path.cwd() / \"data/ECGData/\").glob(\"*.csv\")), total = num_examples-1):\n",
    "        \n",
    "        # get name of file.\n",
    "        fname = str(file.name).split('.')[0]\n",
    "        \n",
    "        # get row number in Diagnostics.xlsx for this file name.\n",
    "        j = diagnostics.index[diagnostics[\"FileName\"] == fname].item()\n",
    "        \n",
    "        # make one-hot vector for condition names.\n",
    "        for b in diagnostics[\"Beat\"][j].split(' '):\n",
    "            if b != \"NONE\":\n",
    "                k = cond_map[b]\n",
    "                Y[i,k] = 1\n",
    "        \n",
    "        X[:, i, :] = pd.read_csv(file).to_numpy()\n",
    "        if i >= num_examples - 1:\n",
    "            break\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    Y = torch.from_numpy(Y).type(torch.FloatTensor)\n",
    "    X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "    \n",
    "    return X, Y, fnames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ecg(y, n_leads = 12):\n",
    "    fig, axs = plt.subplots(n_leads, figsize=(8, 8))\n",
    "    plt.xlabel(\"Seconds\")\n",
    "    plt.ylabel(\"Microvolts\")\n",
    "    plt.xlim([0, 10])\n",
    "    \n",
    "    x = np.linspace(0, 10, 10 * 500) # 500Hz\n",
    "    for i in range(n_leads):\n",
    "        print(y.shape)\n",
    "        axs[i].plot(x, y[i,:].numpy(), color='black')\n",
    "        \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "d8c6afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss for VAE\n",
    "def vae_loss(X, dec_out, z_mu, z_log_var, alpha=1):\n",
    "    \n",
    "    # Reconstruction loss.\n",
    "    recl = mse(X, dec_out)\n",
    "    \n",
    "    # KL Divergence.\n",
    "    dkl = -0.5 * torch.sum(1 + z_log_var - torch.pow(z_mu, 2) - torch.exp(z_log_var),\n",
    "                          axis=1)\n",
    "    dkl = dkl.mean()\n",
    "    \n",
    "    print(\"recl:\", recl, \"dkl:\", dkl)\n",
    "    \n",
    "    return alpha * recl + dkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "5a3ae31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_cnn(X):\n",
    "    \"\"\"\n",
    "    Reshapes the data from (X_LEN, B_SZ, X_DIM) (e.g. (5000, 10, 12))\n",
    "    to (B_SZ, 1, X_DIM, X_LEN), because we have 1 channel.\n",
    "    \"\"\"\n",
    "    X_LEN, B_SZ, X_DIM = X.shape\n",
    "    X = torch.permute(X, (1, 2, 0))\n",
    "    X = torch.reshape(X, (B_SZ, 1, X_DIM, X_LEN))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ce460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10646 Examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████                        | 3762/10645 [00:17<00:30, 222.59it/s]"
     ]
    }
   ],
   "source": [
    "X,Y = load_data()\n",
    "\n",
    "X_DIM = 12\n",
    "X_LEN = 5000\n",
    "N_EX = X.shape[1]\n",
    "B_SZ = 50\n",
    "\n",
    "# Shrink the data in accordance with above.\n",
    "X = X[0:X_LEN,0:N_EX,0:X_DIM]\n",
    "\n",
    "def reshape_for_cnn(X):\n",
    "    \"\"\"\n",
    "    Reshapes the data from (X_LEN, B_SZ, X_DIM) (e.g. (5000, 10, 12))\n",
    "    to (B_SZ, 1, X_DIM, X_LEN), because we have 1 channel.\n",
    "    \"\"\"\n",
    "    X_LEN, B_SZ, X_DIM = X.shape\n",
    "    X = torch.permute(X, (1, 2, 0))\n",
    "    X = torch.reshape(X, (B_SZ, 1, X_DIM, X_LEN))\n",
    "    return X\n",
    "\n",
    "X = reshape_for_cnn(X)\n",
    "\n",
    "# Model and optimizer.\n",
    "vae = VariationalAutoEncoder()\n",
    "cl = Classifier()\n",
    "bce = torch.nn.BCELoss()\n",
    "mse = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr = 1e-4, weight_decay = 1e-8)\n",
    "\n",
    "# Place on GPU\n",
    "# mps_device = torch.device(\"mps\")\n",
    "\n",
    "### Train\n",
    "N_EPOCHS = 1\n",
    "print(\"Starting training...\")\n",
    "for e_i in range(N_EPOCHS):\n",
    "    \n",
    "    # shuffle the indices\n",
    "    idx = np.arange(N_EX)\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    # iterate over batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = N_EX // B_SZ\n",
    "    for b_i in range(num_batches):\n",
    "        \n",
    "        # Make batch\n",
    "        X_batch = X[idx[b_i * B_SZ : (b_i + 1) * B_SZ],:,:,:]\n",
    "        Y_batch = Y[idx[b_i * B_SZ : (b_i + 1) * B_SZ],:]\n",
    "  \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        #dec_out, z_mu, z_log_var = vae.forward(X_batch)\n",
    "        #Y_pred = cl.forward(X_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        #loss = vae_loss(X_batch, dec_out, z_mu, z_log_var)\n",
    "        #loss = bce(Y_pred, Y_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss.item())\n",
    "        \n",
    "    print(\"Epoch #{0} Loss:{1}\".format(e_i + 1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
