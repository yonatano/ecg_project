{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cf082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_impulse_train(x):\n",
    "    \n",
    "    # standardize the signal.\n",
    "    x = x / (2 * np.std(x))\n",
    "    \n",
    "    # raise to a power.\n",
    "    x = np.power(x, 4)\n",
    "    \n",
    "    # smooth the signal.\n",
    "    hw = np.hamming(11)\n",
    "    x = np.convolve(x, hw, 'same')\n",
    "    \n",
    "    # threshold\n",
    "    # x[x < 0.5 * np.max(x)] = 0\n",
    "    \n",
    "    # restore relative amplitudes. \n",
    "    x = np.log(1 + x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def make_impulse_trains(x):\n",
    "    \"\"\"\n",
    "    Generates an impulse train for each example.\n",
    "    ----\n",
    "    x: input of shape (N, H, W)\n",
    "    \n",
    "    Returns impulse_trains: a tensor of impulse trains for all \n",
    "    examples and ECG channels of shape (N, H, W).\n",
    "    \"\"\"\n",
    "    N, H, W = x.shape\n",
    "    impulse_trains = torch.zeros((N, H, W))\n",
    "    for i in range(N): # Loop over batch elements.\n",
    "        for h in range(H): # Loop over ECG channels (height).\n",
    "            im_tr = signal_to_impulse_train(x[i,h].numpy())\n",
    "            im_tr = torch.from_numpy(im_tr).type(torch.FloatTensor)\n",
    "            impulse_trains[i, h] = im_tr\n",
    "    \n",
    "    return impulse_trains\n",
    "\n",
    "class EncoderResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, kernel_sz, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        h_pad = int(kernel_sz / 2.0)\n",
    "        \n",
    "        self.conv_1 = torch.nn.Sequential(\n",
    "            #torch.nn.BatchNorm2d(in_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_ch, out_ch, kernel_size=(1, kernel_sz), padding='same', stride=(1,1)),\n",
    "        )\n",
    "        \n",
    "        self.conv_2 = torch.nn.Sequential(\n",
    "            #torch.nn.BatchNorm2d(out_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_ch, out_ch, kernel_size=(1, kernel_sz), padding='same', stride=(1,1)),   \n",
    "        )\n",
    "        \n",
    "        # 1x1 Convolution with a horizontal stride of 2.\n",
    "        # We need two of these because if in_ch < out_ch, we can't downsample both the output and the input (identity)\n",
    "        # using the same layer.\n",
    "        self.downsampler_id = torch.nn.Conv2d(in_ch, in_ch, kernel_size = (1, 1), padding=(0, 0), stride=(1,2))\n",
    "        self.downsampler_out = torch.nn.Conv2d(out_ch, out_ch, kernel_size = (1, 1), padding=(0, 0), stride=(1,2))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Store X - to be added to the output later as a skip connection.\n",
    "        Id = X\n",
    "        \n",
    "        # Apply first convolution.\n",
    "        X = self.conv_1(X)\n",
    "        \n",
    "        # Apply second convolution.\n",
    "        X = self.conv_2(X)\n",
    "    \n",
    "        # Downsample.\n",
    "        if self.downsample:\n",
    "            X = self.downsampler_out(X)\n",
    "            # we also need to downsample the original input so we can add the two.\n",
    "            Id = self.downsampler_id(Id) \n",
    "        \n",
    "        # Skip Connection.\n",
    "        # If this is a residual block in which the number of channels increases,\n",
    "        # add the input to the first in_ch dimensions of the output.\n",
    "        # TODO: add twice if out_ch = 2 * in_ch.\n",
    "        if self.in_ch < self.out_ch:\n",
    "            pd = self.out_ch - self.in_ch\n",
    "            X = X + torch.nn.functional.pad(Id, (0,0, 0,0, 0,pd, 0,0))\n",
    "        else:\n",
    "            X = X + Id\n",
    "        \n",
    "        Apply Dropout.\n",
    "        X = self.dropout(X)\n",
    "        \n",
    "        return X\n",
    "            \n",
    "class DecoderResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, kernel_sz, upsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.upsample = upsample\n",
    "        \n",
    "        h_pad = int(kernel_sz / 2.0)\n",
    "        \n",
    "        self.conv_1 = torch.nn.Sequential(\n",
    "            #torch.nn.BatchNorm2d(in_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(in_ch, out_ch, kernel_size=(1, kernel_sz), padding=(0,h_pad), stride=(1,1)),   \n",
    "        )\n",
    "        \n",
    "        self.conv_2 = torch.nn.Sequential(\n",
    "            #torch.nn.BatchNorm2d(out_ch),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(out_ch, out_ch, kernel_size=(1, kernel_sz), padding=(0, h_pad), stride=(1,1)),\n",
    "        )\n",
    "        \n",
    "        # 1x1 Convolution with a horizontal stride of 2.\n",
    "        self.upsampler = torch.nn.ConvTranspose2d(out_ch, out_ch, kernel_size = (1, 1), padding=(0, 0), stride=(1,2), output_padding=(0,1))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # Apply first convolution.\n",
    "        X = self.conv_1(X)\n",
    "        \n",
    "        # Apply second convolution.\n",
    "        X = self.conv_2(X)\n",
    "        \n",
    "        # Upsample.\n",
    "        if self.upsample:\n",
    "            X = self.upsampler(X)\n",
    "        \n",
    "        # Apply Dropout.\n",
    "        # X = self.dropout(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "class CNNVariationalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder.\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 1),\n",
    "            EncoderResidualBlock(16, 16, 19, downsample=False),\n",
    "            EncoderResidualBlock(16, 16, 19, downsample=True),\n",
    "            EncoderResidualBlock(16, 32, 19, downsample=True),\n",
    "            EncoderResidualBlock(32, 48, 19, downsample=True),\n",
    "            EncoderResidualBlock(48, 64, 19, downsample=True),\n",
    "            EncoderResidualBlock(64, 64, 19, downsample=True),\n",
    "            EncoderResidualBlock(64, 80, 9, downsample=False),\n",
    "            EncoderResidualBlock(80, 80, 9, downsample=False),\n",
    "            EncoderResidualBlock(80, 80, 9, downsample=False),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Decoder.\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(60, 5120), # Upscale latent vector.\n",
    "            torch.nn.Unflatten(1, (80, 1, 64)),\n",
    "            DecoderResidualBlock(80, 80, 9, upsample=False),\n",
    "            DecoderResidualBlock(80, 80, 9, upsample=False),\n",
    "            DecoderResidualBlock(80, 64, 9, upsample=False),\n",
    "            DecoderResidualBlock(64, 64, 19, upsample=True),\n",
    "            DecoderResidualBlock(64, 48, 19, upsample=True),\n",
    "            DecoderResidualBlock(48, 32, 19, upsample=True),\n",
    "            DecoderResidualBlock(32, 16, 19, upsample=True),\n",
    "            DecoderResidualBlock(16, 16, 19, upsample=True),\n",
    "            DecoderResidualBlock(16, 16, 19, upsample=False),\n",
    "            torch.nn.ConvTranspose2d(16, 1, 1)\n",
    "        )\n",
    "        \n",
    "        # Layers for predicting parameters of q(z|x).\n",
    "        self.latent_mu = torch.nn.Linear(5120, 60)\n",
    "        self.latent_log_var = torch.nn.Linear(5120, 60)\n",
    "    \n",
    "    def sample_gaussian(self, mu, log_var):\n",
    "        B_SZ = mu.shape[0]\n",
    "        eps = torch.randn((B_SZ, 60))\n",
    "        return mu + eps * torch.exp(log_var / 2.0) # multiply by std. dev.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Reshape x before passing thru CNN.\n",
    "        x = x.unsqueeze(1) # (N, H, W) => (N, 1, H, W)\n",
    "        \n",
    "        # Get encoder output.\n",
    "        enc_out = self.encoder(x)\n",
    "        \n",
    "        # Predict parameters of q(z|x).\n",
    "        z_mu = self.latent_mu(enc_out)\n",
    "        z_log_var = self.latent_log_var(enc_out)\n",
    "        \n",
    "        # Reparameterization \"trick\". \n",
    "        # Get a multivariate gaussian with means z_mu and variances z_var * I.\n",
    "        z = self.sample_gaussian(z_mu, z_log_var)\n",
    "        \n",
    "        # Pass latent sample through decoder.\n",
    "        dec_out = self.decoder.forward(z)\n",
    "        \n",
    "        return dec_out, z_mu, z_log_var\n",
    "    \n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn((n, 60))\n",
    "            dec_out = self.decoder.forward(z)\n",
    "            return dec_out\n",
    "    \n",
    "class ModifiedVariationalAutoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "        self.latent_dim = 60\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 1),\n",
    "            EncoderResidualBlock(16, 16, 19, downsample=False),\n",
    "            EncoderResidualBlock(16, 16, 19, downsample=True),\n",
    "            EncoderResidualBlock(16, 32, 19, downsample=True),\n",
    "            EncoderResidualBlock(32, 48, 19, downsample=True),\n",
    "            EncoderResidualBlock(48, 64, 19, downsample=True),\n",
    "            EncoderResidualBlock(64, 64, 19, downsample=True),\n",
    "            EncoderResidualBlock(64, 64, 19, downsample=True),\n",
    "            EncoderResidualBlock(64, 80, 9, downsample=False),\n",
    "            EncoderResidualBlock(80, 80, 9, downsample=False),\n",
    "            EncoderResidualBlock(80, 80, 9, downsample=False),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.latent_dim, 6320), # Upscale latent vector.\n",
    "            torch.nn.Unflatten(1, (80, 1, 79)),\n",
    "            DecoderResidualBlock(80, 80, 9, upsample=False),\n",
    "            DecoderResidualBlock(80, 80, 9, upsample=False),\n",
    "            DecoderResidualBlock(80, 64, 9, upsample=False),\n",
    "            DecoderResidualBlock(64, 64, 19, upsample=False),\n",
    "            DecoderResidualBlock(64, 64, 19, upsample=False),\n",
    "            DecoderResidualBlock(64, 48, 19, upsample=True),\n",
    "            DecoderResidualBlock(48, 32, 19, upsample=True),\n",
    "            DecoderResidualBlock(32, 16, 19, upsample=True),\n",
    "            DecoderResidualBlock(16, 16, 19, upsample=False),\n",
    "            torch.nn.ConvTranspose2d(16, 1, 1), # 625\n",
    "        )\n",
    "        \n",
    "        # Layers for predicting parameters of q(z|x).\n",
    "        self.linear_mu = torch.nn.Linear(2560, self.latent_dim)\n",
    "        torch.nn.init.constant_(self.linear_mu.weight, 0)\n",
    "        self.linear_var =  torch.nn.Linear(2560, self.latent_dim)\n",
    "        torch.nn.init.constant_(self.linear_var.weight, 0)\n",
    "    \n",
    "    def apply_filters(self, f, im):\n",
    "        \"\"\"\n",
    "        f: tensor of filters of shape (N, 1, H, W)\n",
    "        im: tensor of impulse trains of shape (N, H, W)\n",
    "        \"\"\"\n",
    "        N, H, W = im.shape\n",
    "        outputs = torch.zeros((0, H, W), device=self.device)\n",
    "        for i in range(N):\n",
    "            \n",
    "            out_i = torch.zeros((0, W), device=self.device) # Build up conv. outputs across height dim.\n",
    "            for h in range(H):\n",
    "\n",
    "                # Convolve the filter at height h with the impulse train at height h.\n",
    "                out = torch.nn.functional.conv1d(im[i:i+1,h].unsqueeze(1), # Expects (N, C, W) - add C dim.\n",
    "                                                  f[i:i+1,:,h],\n",
    "                                                 padding='same')\n",
    "                \n",
    "                out = out.squeeze(0) # remove fake batch dim.\n",
    "                \n",
    "                # \"append\" this result to out_i along the H dim.\n",
    "                out_i = torch.cat((out_i, out)) \n",
    "            \n",
    "            # Add fake batch dimension so we can concat to outputs.\n",
    "            out_i = out_i.unsqueeze(0) \n",
    "            \n",
    "            # \"append\" this result to outputs along the N dim.\n",
    "            outputs = torch.cat((outputs, out_i)) \n",
    "        \n",
    "        return outputs \n",
    "    \n",
    "    def sample_gaussian(self, mu, log_var):\n",
    "        N, D = mu.shape\n",
    "        eps = torch.randn((N, D), device=self.device)\n",
    "        return mu + eps * torch.exp(log_var / 2.0) # multiply by std. dev.\n",
    "    \n",
    "    def forward(self, x, im):\n",
    "        \"\"\"\n",
    "        x: input of shape (N, H, W)\n",
    "        im: impulse trains of shape (N, H, W)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reshape x before passing thru CNN.\n",
    "        x = x.unsqueeze(1) # (N, H, W) => (N, 1, H, W)\n",
    "\n",
    "        # Get encoder output.\n",
    "        eo = self.encoder(x)\n",
    "        \n",
    "        # Predict parameters of q(z|x).\n",
    "        z_mu = self.linear_mu(eo)\n",
    "        z_log_var = self.linear_var(eo)\n",
    "        \n",
    "        # Sample a multivariate gaussian via the reparameterization \"trick\".\n",
    "        z = self.sample_gaussian(z_mu, z_log_var)\n",
    "        \n",
    "        # Get filter from decoder.\n",
    "        f = self.decoder(z)\n",
    "        \n",
    "        # Convolve impulse trains with filters.\n",
    "        out = self.apply_filters(f, im)\n",
    "        \n",
    "        return out, z_mu, z_log_var\n",
    "    \n",
    "    def generate(self, im):\n",
    "        \"\"\"\n",
    "        im: tensor of shapes (N, H, W)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            N, H, W = im.shape\n",
    "            \n",
    "            # Sample latent vector.\n",
    "            z = torch.randn((N, self.latent_dim))\n",
    "\n",
    "            # Get filter from decoder.\n",
    "            f = self.decoder(z)\n",
    "            \n",
    "            # Convolve impulse trains with filters.\n",
    "            out = self.apply_filters(f, im)\n",
    "            \n",
    "            return out\n",
    "    \n",
    "def vae_loss(x, r, z_mu, z_log_var, alpha=1):\n",
    "    \n",
    "    # Reconstruction loss.\n",
    "    recl = torch.nn.functional.mse_loss(x, r, reduction='mean')\n",
    "    \n",
    "    # KL Divergence.\n",
    "    dkl = -0.5 * torch.sum(1 + z_log_var - torch.pow(z_mu, 2) - torch.exp(z_log_var), axis=1)\n",
    "    dkl = dkl.mean()\n",
    "    \n",
    "    return recl + alpha * dkl, recl, dkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### frange_cycle_linear is from Fu et. al. https://arxiv.org/abs/1903.10145\n",
    "def frange_cycle_linear(start, stop, n_epoch, n_cycle=4, ratio=0.5):\n",
    "    L = np.ones(n_epoch)\n",
    "    period = n_epoch/n_cycle\n",
    "    step = (stop-start)/(period*ratio) # linear schedule\n",
    "\n",
    "    for c in range(n_cycle):\n",
    "\n",
    "        v , i = start , 0\n",
    "        while v <= stop and (int(i+c*period) < n_epoch):\n",
    "            L[int(i+c*period)] = v\n",
    "            v += step\n",
    "            i += 1\n",
    "    return L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b92949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(mdl, x, opt, batch_size=25, num_epochs=10, save_checkpoints=0, checkpoint_dir=\"\"):\n",
    "    \n",
    "    N, H, W = x.shape\n",
    "    \n",
    "    num_batches = N // batch_size\n",
    "        \n",
    "    alphas = frange_cycle_linear(0.0, 1.0, num_epochs, n_cycle = 4, ratio=0.5)\n",
    "    \n",
    "    for e_i in range(num_epochs):\n",
    "    \n",
    "        # shuffle the indices\n",
    "        idx = np.arange(N)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        # iterate over batches\n",
    "        for b_i in range(num_batches):\n",
    "\n",
    "            # Make batch.\n",
    "            batch_idx = idx[b_i * batch_size : (b_i + 1) * batch_size]\n",
    "            x_batch = x[batch_idx,:,:]\n",
    "\n",
    "            # Zero gradients.\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass.\n",
    "            out, z_mu, z_log_var = mdl.forward(x_batch)\n",
    "\n",
    "            # Calculate loss.\n",
    "            alpha = alphas[e_i]\n",
    "            loss, recl, dkl = vae_loss(x_batch, out, z_mu, z_log_var, alpha=alpha)\n",
    "            print(loss.item(), recl.item(), dkl.item(), alpha)\n",
    "\n",
    "            # Compute gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(mdl.parameters(), max_norm=0.1, norm_type=\"inf\")\n",
    "            \n",
    "            # Update parameters.\n",
    "            opt.step()\n",
    "\n",
    "        # Print Loss.\n",
    "        print(\"Epoch #{0} Loss:{1}\".format(e_i + 1, loss.item()))\n",
    "        \n",
    "        # Save model checkpoint.\n",
    "        if ((save_checkpoints > 0 and (e_i + 1) % save_checkpoints == 0) or \n",
    "            (save_checkpoints == -1 and e_i + 1 == num_epochs)):\n",
    "            \n",
    "            PATH = checkpoint_dir + \"mdl_epoch_{1}.pt\".format(checkpoint_dir, e_i + 1)\n",
    "            torch.save({\n",
    "                'epoch': e_i + 1,\n",
    "                'model_state_dict': mdl.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                }, PATH)\n",
    "\n",
    "def train_modified_vae(mdl, x, im, opt, batch_size=25, num_epochs=10, save_checkpoints=0, checkpoint_dir=\"\"):\n",
    "    \"\"\"\n",
    "    If save_checkpoints is 0, will not save checkpoints.\n",
    "    If save_checkpoints is -1, will save only at the last epoch. \n",
    "    \"\"\"\n",
    "    \n",
    "    N, H, W = x.shape\n",
    "    \n",
    "    num_batches = N // batch_size\n",
    "        \n",
    "    #alphas = frange_cycle_linear(0.0, 1.0, num_epochs * num_batches, n_cycle = 4, ratio=0.5)\n",
    "    alphas = frange_cycle_linear(0.0, 1.0, num_epochs, n_cycle = 4, ratio=0.5)\n",
    "    \n",
    "    for e_i in range(num_epochs):\n",
    "    \n",
    "        # shuffle the indices\n",
    "        idx = np.arange(N)\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        # iterate over batches\n",
    "        for b_i in range(num_batches):\n",
    "\n",
    "            # Make batch.\n",
    "            batch_idx = idx[b_i * batch_size : (b_i + 1) * batch_size]\n",
    "            x_batch = x[batch_idx,:,:]\n",
    "            im_batch = im[batch_idx,:,:]\n",
    "\n",
    "            # Zero gradients.\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass.\n",
    "            out, z_mu, z_log_var = mdl.forward(x_batch, im_batch)\n",
    "\n",
    "            # Calculate loss.\n",
    "            #alpha = alphas[num_batches * e_i + b_i]\n",
    "            alpha = alphas[e_i]\n",
    "            loss, recl, dkl = vae_loss(x_batch, out, z_mu, z_log_var, alpha=alpha)\n",
    "            print(loss.item(), recl.item(), dkl.item(), alpha)\n",
    "\n",
    "            # Compute gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(mdl.parameters(), max_norm=0.1, norm_type=\"inf\")\n",
    "            \n",
    "            # Update parameters.\n",
    "            opt.step()\n",
    "\n",
    "        # Print Loss.\n",
    "        print(\"Epoch #{0} Loss:{1}\".format(e_i + 1, loss.item()))\n",
    "        \n",
    "        # Save model checkpoint.\n",
    "        if ((save_checkpoints > 0 and (e_i + 1) % save_checkpoints == 0) or \n",
    "            (save_checkpoints == -1 and e_i + 1 == num_epochs)):\n",
    "            \n",
    "            PATH = checkpoint_dir + \"mdl_epoch_{1}.pt\".format(checkpoint_dir, e_i + 1)\n",
    "            torch.save({\n",
    "                'epoch': e_i + 1,\n",
    "                'model_state_dict': mdl.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                }, PATH)\n",
    "            \n",
    "def load_model(PATH):\n",
    "    \n",
    "    mdl = ModifiedVariationalAutoencoder()\n",
    "    opt = torch.optim.Adam(mdl.parameters(), lr = 1e-4, weight_decay = 1e-8)\n",
    "    \n",
    "    checkpoint = torch.load(PATH)\n",
    "    mdl.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return mdl, opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train Modified VAE\n",
    "\n",
    "# Load data.\n",
    "x, y = load_data(1000)\n",
    "x = torch.permute(x, (1, 2, 0)) # (W, N, H) => (N, H, W)\n",
    "x = x[:,:1,:2048] # first lead\n",
    "\n",
    "# Skip problematic files.\n",
    "skip = []\n",
    "for i in range(x.shape[0]):\n",
    "    if int(np.linalg.norm(x[i,0])) == 0:\n",
    "        skip.append(i)\n",
    "skip_idxs = [i for i in range(x.shape[0]) if i not in skip]\n",
    "x = x[skip_idxs]\n",
    "\n",
    "# Get impulse trains from inputs.\n",
    "im = make_impulse_trains(x)\n",
    "\n",
    "# Build model.\n",
    "mdl = ModifiedVariationalAutoencoder()\n",
    "\n",
    "# Move to GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = x.to(mps_device)\n",
    "    im = im.to(mps_device)\n",
    "    mdl = mdl.to(mps_device)\n",
    "    mdl.device = mps_device\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "# Train.\n",
    "opt = torch.optim.Adam(mdl.parameters(), lr = 1e-5, weight_decay = 1e-9)\n",
    "PATH = \"/Users/yonatano/Documents/Courses/CS 230/Project/models/vae_run_4/\"\n",
    "train_modified_vae(mdl, x, im, opt, batch_size=1, num_epochs=20000, save_checkpoints=0, checkpoint_dir=PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
