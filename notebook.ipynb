{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538cf082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8954da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class encoder(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Initialize an RNN encoder:\n",
    "#     x_len: The length of an input sequence. Each ECG example has length 5000.\n",
    "#     x_dim: The dimension of the input sequence at each time step. At each time step \n",
    "#         we have 12 numbers (one for each lead), so this is 12.\n",
    "#     h_dim: The number of hidden units in the RNN.\n",
    "#     n_lyr: The number of layers to use in the RNN. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, x_len, x_dim, h_dim, n_lyr):\n",
    "#         super().__init__()\n",
    "#         # self.bn = torch.nn.BatchNorm1d(x_dim)\n",
    "#         self.rnn = torch.nn.RNN(x_dim, h_dim, n_lyr)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Compute the forward pass. \n",
    "#     x: Input tensor of size (x_len, N, x_dim), where N is the number of examples in the batch.\n",
    "    \n",
    "#     Returns z, a tensor of size (x_len, N, h_dim), containing the hidden states computed at each time step.\n",
    "#     \"\"\"\n",
    "#     def forward(self, x):\n",
    "#         # x = self.bn(torch.permute(x, (1, 2, 0))) # (L, N, C) -> (N, C, L)\n",
    "#         # x = torch.permute(x, (2, 0, 1)) # (N, C, L) -> (L, N, C)\n",
    "#         z, _ = self.rnn(x)\n",
    "#         return z\n",
    "\n",
    "# class decoder(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, x_len, x_dim, h_dim, n_lyr):\n",
    "#         super().__init__()\n",
    "#         # self.bn = torch.nn.BatchNorm1d(h_dim)\n",
    "#         self.rnn = torch.nn.RNN(h_dim, x_dim, n_lyr)\n",
    "        \n",
    "#     def forward(self, z):\n",
    "#         # z = self.bn(torch.permute(z, (1, 2, 0))) # (L, N, C) -> (N, C, L)\n",
    "#         # z = torch.permute(z, (2, 0, 1)) # (N, C, L) -> (L, N, C)\n",
    "#         r, _ = self.rnn(z)\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80258386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class encoder(torch.nn.Module):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Initialize an RNN encoder:\n",
    "#     x_len: The length of an input sequence. Each ECG example has length 5000.\n",
    "#     x_dim: The dimension of the input sequence at each time step. At each time step \n",
    "#         we have 12 numbers (one for each lead), so this is 12.\n",
    "#     h_dim: The number of hidden units in the RNN. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, x_len, x_dim, h_dim):\n",
    "#         super().__init__()\n",
    "#         self.x_len = x_len\n",
    "#         self.x_dim = x_dim\n",
    "#         self.h_dim = h_dim\n",
    "#         #self.bn = torch.nn.BatchNorm1d(x_dim)\n",
    "#         self.rnn_cell = torch.nn.RNNCell(x_dim, h_dim, nonlinearity=\"relu\")\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Compute the forward pass. \n",
    "#     x: Input tensor of size (x_len, N, x_dim), where N is the number of examples in the batch.\n",
    "    \n",
    "#     Returns a tensor of size (N, h_dim), containing the last hidden state.\n",
    "#     \"\"\"\n",
    "#     def forward(self, x):\n",
    "#         # Create a tensor to append the hidden states to: (1, batch_size, h_dim)\n",
    "#         # The first dimension starts at 1 because we start with one null hidden state.\n",
    "#         hx_s = torch.zeros((1, x.shape[1], self.h_dim))\n",
    "\n",
    "#         for t in range(1, self.x_len):\n",
    "#             hx = self.rnn_cell(x[t,:,:], hx_s[t-1,:,:])\n",
    "#             hx = torch.reshape(hx, (1, x.shape[1], self.h_dim))\n",
    "#             hx_s = torch.cat((hx_s, hx), 0)\n",
    "\n",
    "#         return hx_s[-1,:,:]\n",
    "\n",
    "# class decoder(torch.nn.Module):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Initialize an RNN decoder:\n",
    "#     x_len: The length of an input sequence. Each ECG example has length 5000.\n",
    "#     x_dim: The dimension of the input sequence at each time step. At each time step \n",
    "#         we have 12 numbers (one for each lead), so this is 12.\n",
    "#     h_dim: The number of hidden units in the RNN. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, x_len, x_dim, h_dim):\n",
    "#         super().__init__()\n",
    "#         self.x_len = x_len\n",
    "#         self.x_dim = x_dim\n",
    "#         self.h_dim = h_dim\n",
    "#         #self.bn = torch.nn.BatchNorm1d(h_dim)\n",
    "#         self.rnn_cell = torch.nn.RNNCell(x_dim, h_dim, nonlinearity=\"relu\")\n",
    "#         self.linear = torch.nn.Linear(h_dim, x_dim)\n",
    "        \n",
    "#     def forward(self, hx):\n",
    "#         batch_size = hx.shape[0]\n",
    "        \n",
    "#         # Create a tensor to append the hidden states to: (0, batch_size, h_dim)\n",
    "#         hx_s = torch.zeros((0, batch_size, self.h_dim))\n",
    "\n",
    "#         # Append the last hidden state of the encoder as our 0th hidden state.\n",
    "#         hx = torch.reshape(hx, (1, batch_size, self.h_dim))\n",
    "#         hx_s = torch.cat((hx_s, hx), 0)\n",
    "\n",
    "#         # Create a tensor to store the decoder outputs.\n",
    "#         outputs = torch.ones((1, batch_size, self.x_dim))\n",
    "        \n",
    "#         for t in range(1, self.x_len+1):\n",
    "#             # Get the next hidden state from the decoder.\n",
    "#             hx = self.rnn_cell(outputs[t-1,:,:], hx_s[t-1,:,:])\n",
    "#             hx = torch.reshape(hx, (1, batch_size, self.h_dim))\n",
    "#             hx_s = torch.cat((hx_s, hx), 0)\n",
    "  \n",
    "#             # Get the next output by applying a linear layer. \n",
    "#             out = self.linear(hx_s[t,:,:])\n",
    "#             out = torch.reshape(out, (1, batch_size, self.x_dim))\n",
    "#             outputs = torch.cat((outputs, out), 0)\n",
    "        \n",
    "#         # print(\"hx_s: \", hx_s)\n",
    "        \n",
    "#         return outputs[1:self.x_len+1,:,:]\n",
    "    \n",
    "# class autoencoder(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, x_len = 5000, x_dim = 12, h_dim = 6):\n",
    "#         super().__init__()\n",
    "#         self.enc = encoder(x_len, x_dim, h_dim)\n",
    "#         self.dec = decoder(x_len, x_dim, h_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         hx = self.enc(x)\n",
    "#         r = self.dec(hx)\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2e6ec7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class encoder(torch.nn.Module):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Initialize an RNN encoder:\n",
    "#     x_len: The length of an input sequence. Each ECG example has length 5000.\n",
    "#     x_dim: The dimension of the input sequence at each time step. At each time step \n",
    "#         we have 12 numbers (one for each lead), so this is 12.\n",
    "#     h_dim: The number of hidden units in the RNN. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, x_len, x_dim, h_dim, n_lyr):\n",
    "#         super().__init__()\n",
    "#         self.x_len = x_len\n",
    "#         self.x_dim = x_dim\n",
    "#         self.h_dim = h_dim\n",
    "#         self.n_lyr = n_lyr\n",
    "        \n",
    "#         # add RNN layers. First layer takes x_dim input vectors. Remaining layers\n",
    "#         # take hidden states as input.\n",
    "#         self.rnn_cells = [torch.nn.RNNCell(x_dim, h_dim, nonlinearity=\"relu\")]\n",
    "#         for _ in range(n_lyr-1):\n",
    "#             cell = torch.nn.RNNCell(h_dim, h_dim, nonlinearity=\"relu\")\n",
    "#             self.rnn_cells.append(cell)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     x_t: Input at some time-step of shape (batch_size, x_dim).\n",
    "#     hx_s: Hidden states at previous time-step of shape (n_lyr, batch_size, h_dim).\n",
    "    \n",
    "#     returns hx_s_t: a tensor containing the next hidden states for each layer of shape (n_lyr, batch_size, h_dim).\n",
    "#     \"\"\"\n",
    "#     def rnn_forward(self, x_t, hx_s):\n",
    "        \n",
    "#         batch_size = x_t.shape[0]\n",
    "        \n",
    "#         hx_s_t = torch.zeros((0, batch_size, self.h_dim)) # hidden states\n",
    "#         in_s_t = torch.zeros((0, batch_size, self.h_dim)) # layer inputs\n",
    "        \n",
    "#         # Pass x_t through the first RNN layer.\n",
    "#         hx_t = self.rnn_cells[0](x_t, hx_s[0,:,:])\n",
    "#         hx_t = torch.reshape(hx_t, (1, batch_size, self.h_dim))\n",
    "#         hx_s_t = torch.cat((hx_s_t, hx_t)) # store the hidden state\n",
    "#         in_s_t = torch.cat((in_s_t, hx_t)) # store also as an input to the next layer.\n",
    "        \n",
    "#         # Propagate up through the remaining RNN layers.\n",
    "#         for i in range(1, self.n_lyr):\n",
    "#             hx_t = self.rnn_cells[i](in_s_t[i-1,:,:], hx_s[i,:,:])\n",
    "#             hx_t = torch.reshape(hx_t, (1, batch_size, self.h_dim))\n",
    "#             hx_s_t = torch.cat((hx_s_t, hx_t)) # store the hidden state\n",
    "#             in_s_t = torch.cat((in_s_t, hx_t)) # store also as an input to the next layer.\n",
    "            \n",
    "#         return hx_s_t\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Compute the forward pass. \n",
    "#     x: Input tensor of size (x_len, N, x_dim), where N is the number of examples in the batch.\n",
    "    \n",
    "#     Returns a tensor of size (N, h_dim), containing the last hidden state of the last layer.\n",
    "#     \"\"\"\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.shape[1]\n",
    "#         hx_s = torch.zeros((self.n_lyr, batch_size, self.h_dim))\n",
    "#         for t in range(0, self.x_len):\n",
    "#             hx_s = self.rnn_forward(x[t,:,:], hx_s)\n",
    "#         #return hx_s[-1,:,:]\n",
    "#         return hx_s\n",
    "\n",
    "# class decoder(torch.nn.Module):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Initialize an RNN decoder:\n",
    "#     x_len: The length of an input sequence. Each ECG example has length 5000.\n",
    "#     x_dim: The dimension of the input sequence at each time step. At each time step \n",
    "#         we have 12 numbers (one for each lead), so this is 12.\n",
    "#     h_dim: The number of hidden units in the RNN. \n",
    "#     \"\"\"\n",
    "#     def __init__(self, x_len, x_dim, h_dim, n_lyr):\n",
    "#         super().__init__()\n",
    "#         self.x_len = x_len\n",
    "#         self.x_dim = x_dim\n",
    "#         self.h_dim = h_dim\n",
    "#         self.n_lyr = n_lyr\n",
    "        \n",
    "#         # add RNN layers. First layer takes x_dim input vectors. Remaining layers\n",
    "#         # take hidden states as input.\n",
    "#         self.rnn_cells = [torch.nn.RNNCell(x_dim, h_dim, nonlinearity=\"relu\")]\n",
    "#         for _ in range(n_lyr-1):\n",
    "#             cell = torch.nn.RNNCell(h_dim, h_dim, nonlinearity=\"relu\")\n",
    "#             self.rnn_cells.append(cell)\n",
    "            \n",
    "#         # Linear layer to compute RNN outputs.\n",
    "#         self.linear = torch.nn.Linear(h_dim, x_dim)\n",
    "    \n",
    "#     \"\"\"\n",
    "#     x_t: Input at some time-step of shape (batch_size, x_dim).\n",
    "#     hx_s: Hidden states at previous time-step of shape (n_lyr, batch_size, h_dim).\n",
    "    \n",
    "#     returns hx_s_t: a tensor containing the next hidden states for each layer of shape (n_lyr, batch_size, h_dim).\n",
    "#     \"\"\"\n",
    "#     def rnn_forward(self, x_t, hx_s):\n",
    "        \n",
    "#         batch_size = x_t.shape[0]\n",
    "        \n",
    "#         hx_s_t = torch.zeros((0, batch_size, self.h_dim)) # hidden states\n",
    "#         in_s_t = torch.zeros((0, batch_size, self.h_dim)) # layer inputs\n",
    "        \n",
    "#         # Pass x_t through the first RNN layer.\n",
    "#         hx_t = self.rnn_cells[0](x_t, hx_s[0,:,:])\n",
    "#         hx_t = torch.reshape(hx_t, (1, batch_size, self.h_dim))\n",
    "#         hx_s_t = torch.cat((hx_s_t, hx_t)) # store the hidden state\n",
    "#         in_s_t = torch.cat((in_s_t, hx_t)) # store also as an input to the next layer.\n",
    "        \n",
    "#         # Propagate up through the remaining RNN layers.\n",
    "#         for i in range(1, self.n_lyr):\n",
    "#             hx_t = self.rnn_cells[i](in_s_t[i-1,:,:], hx_s[i,:,:])\n",
    "#             hx_t = torch.reshape(hx_t, (1, batch_size, self.h_dim))\n",
    "#             hx_s_t = torch.cat((hx_s_t, hx_t)) # store the hidden state\n",
    "#             in_s_t = torch.cat((in_s_t, hx_t)) # store also as an input to the next layer.\n",
    "            \n",
    "#         return hx_s_t\n",
    "\n",
    "#     def forward(self, hx):\n",
    "        \n",
    "#         #batch_size = hx.shape[0]\n",
    "#         batch_size = hx.shape[1]\n",
    "        \n",
    "#         # store the outputs.\n",
    "#         outputs = torch.zeros((0, batch_size, self.x_dim)) \n",
    "        \n",
    "#         # initialize the hidden states for each layer.\n",
    "#         #hx_s = torch.zeros((self.n_lyr-1, batch_size, self.h_dim))\n",
    "#         hx_s = hx\n",
    "        \n",
    "#         # prepend hx as our 0th hidden state to the first layer.\n",
    "#         #hx = torch.reshape(hx, (1, batch_size, self.h_dim))\n",
    "#         #hx_s = torch.cat((hx, hx_s), 0) \n",
    "        \n",
    "#         # first input is zeros.\n",
    "#         x_t = torch.zeros((batch_size, x_dim)) \n",
    "        \n",
    "#         for t in range(0, self.x_len):\n",
    "#             # get next hidden states.\n",
    "#             hx_s = self.rnn_forward(x_t, hx_s)\n",
    "            \n",
    "#             # pass the hidden state of the last RNN layer through \n",
    "#             # a linear layer to get the output.\n",
    "#             y_t = self.linear(hx_s[-1,:,:])\n",
    "#             out = torch.reshape(y_t, (1, batch_size, self.x_dim))\n",
    "#             outputs = torch.cat((outputs, out), 0)\n",
    "            \n",
    "#             # the current output is the next input.\n",
    "#             x_t = y_t\n",
    "            \n",
    "#         return outputs\n",
    "\n",
    "# class autoencoder(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, x_len = 5000, x_dim = 12, h_dim = 6, n_lyr = 1):\n",
    "#         super().__init__()\n",
    "#         self.enc = encoder(x_len, x_dim, h_dim, n_lyr)\n",
    "#         self.dec = decoder(x_len, x_dim, h_dim, n_lyr)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         hx = self.enc(x)\n",
    "#         r = self.dec(hx)\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ce61287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, 3, stride=(1,2), padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(8, 16, 3, stride=(1,2), padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, stride=(1,2), padding=1),\n",
    "        )\n",
    "        \n",
    "        self.flatten = torch.nn.Flatten(start_dim=1) # 2688\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2688, 1500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1500, 1500)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.cnn(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.linear(X)\n",
    "        return X\n",
    "        \n",
    "class Decoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1500, 1500),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1500, 2688),\n",
    "        )\n",
    "        \n",
    "        self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size=(32, 12, 7))\n",
    "        \n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(32, 16, 3, stride=(1,2), padding=1, output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(16, 8, 3, stride=(1,2), padding=1, output_padding=(0,0)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(8, 1, 3, stride=(1,2), padding=1, output_padding=(0,1))\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = self.unflatten(X)\n",
    "        X = self.cnn(X)\n",
    "        return X\n",
    "    \n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379f0f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 12, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_0 = torch.randn(15, 1, 12, 50)\n",
    "ae = AutoEncoder()\n",
    "X_1 = ae.forward(X_0)\n",
    "X_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24cd611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1, 12, 50])\n",
      "torch.Size([15, 8, 12, 25])\n",
      "torch.Size([15, 16, 12, 13])\n",
      "torch.Size([15, 32, 12, 7])\n"
     ]
    }
   ],
   "source": [
    "conv_1 = torch.nn.Conv2d(1, 8, 3, stride=(1,2), padding=1)\n",
    "conv_2 = torch.nn.Conv2d(8, 16, 3, stride=(1,2), padding=1)\n",
    "conv_3 = torch.nn.Conv2d(16, 32, 3, stride=(1,2), padding=1)\n",
    "\n",
    "X = torch.randn(15, 1, 12, 50)\n",
    "print(X.shape)\n",
    "X = conv_1(X)\n",
    "print(X.shape)\n",
    "X = conv_2(X)\n",
    "print(X.shape)\n",
    "X = conv_3(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49ff7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(n = -1):\n",
    "    data = pd.read_excel(\"data/Diagnostics.xlsx\")\n",
    "    num_examples = data.shape[0] if n == -1 else n\n",
    "    \n",
    "    X = np.zeros((5000, num_examples, 12))\n",
    "    \n",
    "    print(\"Loading\", num_examples, \"Examples.\")\n",
    "    for i, file in tqdm(enumerate((Path.cwd() / \"data/ECGData/\").glob(\"*.csv\")), total = num_examples):\n",
    "        fname = file.name\n",
    "        X[:, i, :] = pd.read_csv(file).to_numpy()\n",
    "        if i >= num_examples - 1:\n",
    "            break\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "    \n",
    "    return X\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ecg(o, r, n_leads = 1):\n",
    "    fig, ax = plt.subplots(figsize=(4, 2))\n",
    "    plt.xlabel(\"Seconds\")\n",
    "    plt.ylabel(\"Microvolts\")\n",
    "    plt.xlim([0, 10])\n",
    "    # plt.ylim([-300, 300])\n",
    "    x = np.linspace(0, 10, 50) # 500Hz\n",
    "    ax.plot(x, org[0,:].numpy(), color='black')\n",
    "    ax.plot(x, rec[0,:].numpy(), color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ae1fc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Rhythm</th>\n",
       "      <th>Beat</th>\n",
       "      <th>PatientAge</th>\n",
       "      <th>Gender</th>\n",
       "      <th>VentricularRate</th>\n",
       "      <th>AtrialRate</th>\n",
       "      <th>QRSDuration</th>\n",
       "      <th>QTInterval</th>\n",
       "      <th>QTCorrected</th>\n",
       "      <th>RAxis</th>\n",
       "      <th>TAxis</th>\n",
       "      <th>QRSCount</th>\n",
       "      <th>QOnset</th>\n",
       "      <th>QOffset</th>\n",
       "      <th>TOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSE_20180113_171327_27000</td>\n",
       "      <td>AFIB</td>\n",
       "      <td>RBBB TWC</td>\n",
       "      <td>85</td>\n",
       "      <td>MALE</td>\n",
       "      <td>117</td>\n",
       "      <td>234</td>\n",
       "      <td>114</td>\n",
       "      <td>356</td>\n",
       "      <td>496</td>\n",
       "      <td>81</td>\n",
       "      <td>-27</td>\n",
       "      <td>19</td>\n",
       "      <td>208</td>\n",
       "      <td>265</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUSE_20180112_073319_29000</td>\n",
       "      <td>SB</td>\n",
       "      <td>TWC</td>\n",
       "      <td>59</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>92</td>\n",
       "      <td>432</td>\n",
       "      <td>401</td>\n",
       "      <td>76</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>215</td>\n",
       "      <td>261</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSE_20180111_165520_97000</td>\n",
       "      <td>SA</td>\n",
       "      <td>NONE</td>\n",
       "      <td>20</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>82</td>\n",
       "      <td>382</td>\n",
       "      <td>403</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>224</td>\n",
       "      <td>265</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSE_20180113_121940_44000</td>\n",
       "      <td>SB</td>\n",
       "      <td>NONE</td>\n",
       "      <td>66</td>\n",
       "      <td>MALE</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>96</td>\n",
       "      <td>456</td>\n",
       "      <td>427</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>219</td>\n",
       "      <td>267</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUSE_20180112_122850_57000</td>\n",
       "      <td>AF</td>\n",
       "      <td>STDD STTC</td>\n",
       "      <td>73</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>114</td>\n",
       "      <td>252</td>\n",
       "      <td>413</td>\n",
       "      <td>68</td>\n",
       "      <td>-40</td>\n",
       "      <td>26</td>\n",
       "      <td>228</td>\n",
       "      <td>285</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FileName Rhythm       Beat  PatientAge  Gender  \\\n",
       "0  MUSE_20180113_171327_27000   AFIB   RBBB TWC          85    MALE   \n",
       "1  MUSE_20180112_073319_29000     SB        TWC          59  FEMALE   \n",
       "2  MUSE_20180111_165520_97000     SA       NONE          20  FEMALE   \n",
       "3  MUSE_20180113_121940_44000     SB       NONE          66    MALE   \n",
       "4  MUSE_20180112_122850_57000     AF  STDD STTC          73  FEMALE   \n",
       "\n",
       "   VentricularRate  AtrialRate  QRSDuration  QTInterval  QTCorrected  RAxis  \\\n",
       "0              117         234          114         356          496     81   \n",
       "1               52          52           92         432          401     76   \n",
       "2               67          67           82         382          403     88   \n",
       "3               53          53           96         456          427     34   \n",
       "4              162         162          114         252          413     68   \n",
       "\n",
       "   TAxis  QRSCount  QOnset  QOffset  TOffset  \n",
       "0    -27        19     208      265      386  \n",
       "1     42         8     215      261      431  \n",
       "2     20        11     224      265      415  \n",
       "3      3         9     219      267      447  \n",
       "4    -40        26     228      285      354  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"data/Diagnostics.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37846778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 Examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████████▌| 99/100 [00:00<00:00, 297.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Loss: 87787.03125\n",
      "Loss: 86931.2109375\n",
      "Loss: 78521.484375\n",
      "Loss: 63384.90625\n",
      "Loss: 70418.578125\n",
      "Loss: 49955.28125\n",
      "Loss: 54884.55859375\n",
      "Loss: 52337.1953125\n",
      "Loss: 42841.328125\n",
      "Loss: 33893.859375\n",
      "Loss: 35723.828125\n",
      "Loss: 26860.0234375\n",
      "Loss: 20947.423828125\n",
      "Loss: 21289.63671875\n",
      "Loss: 19372.44921875\n",
      "Loss: 15161.5380859375\n",
      "Loss: 13553.5908203125\n",
      "Loss: 13186.0771484375\n",
      "Loss: 9559.5693359375\n",
      "Loss: 8548.955078125\n",
      "Loss: 8566.693359375\n",
      "Loss: 6898.73486328125\n",
      "Loss: 5619.7958984375\n",
      "Loss: 5995.958984375\n",
      "Loss: 4843.78857421875\n",
      "Loss: 4179.45458984375\n",
      "Loss: 4500.732421875\n",
      "Loss: 3872.423828125\n",
      "Loss: 3262.544921875\n",
      "Loss: 3476.157958984375\n",
      "Loss: 2840.379638671875\n",
      "Loss: 2455.7255859375\n",
      "Loss: 2556.41552734375\n",
      "Loss: 2151.343017578125\n",
      "Loss: 1970.048095703125\n",
      "Loss: 2050.62109375\n",
      "Loss: 1674.948486328125\n",
      "Loss: 1634.05712890625\n",
      "Loss: 1613.274658203125\n",
      "Loss: 1356.5125732421875\n",
      "Loss: 1382.0635986328125\n",
      "Loss: 1283.5791015625\n",
      "Loss: 1107.309326171875\n",
      "Loss: 1142.684326171875\n",
      "Loss: 1013.1964721679688\n",
      "Loss: 937.027099609375\n",
      "Loss: 957.221435546875\n",
      "Loss: 828.73193359375\n",
      "Loss: 827.4789428710938\n",
      "Loss: 788.8041381835938\n",
      "Loss: 701.9481201171875\n",
      "Loss: 721.0280151367188\n",
      "Loss: 648.794189453125\n",
      "Loss: 623.50244140625\n",
      "Loss: 615.7842407226562\n",
      "Loss: 553.5638427734375\n",
      "Loss: 560.1415405273438\n",
      "Loss: 516.42333984375\n",
      "Loss: 491.5889587402344\n",
      "Loss: 482.2403564453125\n",
      "Loss: 438.724365234375\n",
      "Loss: 439.5378112792969\n",
      "Loss: 410.1888427734375\n",
      "Loss: 398.4404602050781\n",
      "Loss: 389.083251953125\n",
      "Loss: 362.8070983886719\n",
      "Loss: 361.4365234375\n",
      "Loss: 338.0985412597656\n",
      "Loss: 332.8367919921875\n",
      "Loss: 319.5897521972656\n",
      "Loss: 305.7237548828125\n",
      "Loss: 299.8009338378906\n",
      "Loss: 281.676025390625\n",
      "Loss: 277.0950927734375\n",
      "Loss: 262.12213134765625\n",
      "Loss: 254.9281005859375\n",
      "Loss: 245.01307678222656\n",
      "Loss: 235.40521240234375\n",
      "Loss: 229.73239135742188\n",
      "Loss: 219.60838317871094\n",
      "Loss: 215.37057495117188\n",
      "Loss: 205.86526489257812\n",
      "Loss: 201.50057983398438\n",
      "Loss: 193.69227600097656\n",
      "Loss: 188.30641174316406\n",
      "Loss: 181.72918701171875\n",
      "Loss: 175.07496643066406\n",
      "Loss: 169.36004638671875\n",
      "Loss: 162.49497985839844\n",
      "Loss: 157.72021484375\n",
      "Loss: 150.9018096923828\n",
      "Loss: 146.2922821044922\n",
      "Loss: 139.8626708984375\n",
      "Loss: 135.54859924316406\n",
      "Loss: 129.59521484375\n",
      "Loss: 124.93392181396484\n",
      "Loss: 119.14933776855469\n",
      "Loss: 114.70722961425781\n",
      "Loss: 109.18760681152344\n",
      "Loss: 104.8746337890625\n",
      "Loss: 99.8105239868164\n",
      "Loss: 95.6869888305664\n",
      "Loss: 90.79707336425781\n",
      "Loss: 86.70051574707031\n",
      "Loss: 82.33406066894531\n",
      "Loss: 78.4169692993164\n",
      "Loss: 74.1677017211914\n",
      "Loss: 70.56876373291016\n",
      "Loss: 67.0102767944336\n",
      "Loss: 63.54560470581055\n",
      "Loss: 60.259490966796875\n",
      "Loss: 56.98385238647461\n",
      "Loss: 53.893898010253906\n",
      "Loss: 50.88029098510742\n",
      "Loss: 48.06448745727539\n",
      "Loss: 45.48375701904297\n",
      "Loss: 43.14421081542969\n",
      "Loss: 41.017616271972656\n",
      "Loss: 38.33174133300781\n",
      "Loss: 36.201316833496094\n",
      "Loss: 34.10026931762695\n",
      "Loss: 31.7335147857666\n",
      "Loss: 29.677194595336914\n",
      "Loss: 27.745439529418945\n",
      "Loss: 25.9605770111084\n",
      "Loss: 24.43762969970703\n",
      "Loss: 22.995851516723633\n",
      "Loss: 21.560697555541992\n",
      "Loss: 20.237140655517578\n",
      "Loss: 19.106172561645508\n",
      "Loss: 18.039976119995117\n",
      "Loss: 17.13980484008789\n",
      "Loss: 16.505586624145508\n",
      "Loss: 16.103221893310547\n",
      "Loss: 15.908666610717773\n",
      "Loss: 15.2656888961792\n",
      "Loss: 14.022794723510742\n",
      "Loss: 13.175532341003418\n",
      "Loss: 13.089890480041504\n",
      "Loss: 12.727560997009277\n",
      "Loss: 11.93436336517334\n",
      "Loss: 11.528616905212402\n",
      "Loss: 11.467203140258789\n",
      "Loss: 11.07839298248291\n",
      "Loss: 10.495756149291992\n",
      "Loss: 10.202312469482422\n",
      "Loss: 10.155960083007812\n",
      "Loss: 9.85743236541748\n",
      "Loss: 9.434351921081543\n",
      "Loss: 9.23245906829834\n",
      "Loss: 9.083416938781738\n",
      "Loss: 8.79930305480957\n",
      "Loss: 8.590027809143066\n",
      "Loss: 8.392937660217285\n",
      "Loss: 8.192621231079102\n",
      "Loss: 8.028881072998047\n",
      "Loss: 7.812089920043945\n",
      "Loss: 7.633035659790039\n",
      "Loss: 7.5422821044921875\n",
      "Loss: 7.380866527557373\n",
      "Loss: 7.231452465057373\n",
      "Loss: 7.0580925941467285\n",
      "Loss: 6.937317848205566\n",
      "Loss: 6.819519996643066\n",
      "Loss: 6.6808295249938965\n",
      "Loss: 6.565719127655029\n",
      "Loss: 6.455006122589111\n",
      "Loss: 6.385441303253174\n",
      "Loss: 6.282121181488037\n",
      "Loss: 6.183497905731201\n",
      "Loss: 6.135169982910156\n",
      "Loss: 6.025052547454834\n",
      "Loss: 5.9034881591796875\n",
      "Loss: 5.828906059265137\n",
      "Loss: 5.731729984283447\n",
      "Loss: 5.6529927253723145\n",
      "Loss: 5.578904628753662\n",
      "Loss: 5.514262676239014\n",
      "Loss: 5.43563175201416\n",
      "Loss: 5.3761091232299805\n",
      "Loss: 5.321226596832275\n",
      "Loss: 5.251116752624512\n",
      "Loss: 5.22149658203125\n",
      "Loss: 5.162108421325684\n",
      "Loss: 5.114501476287842\n",
      "Loss: 5.05668306350708\n",
      "Loss: 5.008195400238037\n",
      "Loss: 4.960726737976074\n",
      "Loss: 4.922730922698975\n",
      "Loss: 4.902560234069824\n",
      "Loss: 4.903274059295654\n",
      "Loss: 4.970817565917969\n",
      "Loss: 5.042324542999268\n",
      "Loss: 5.1249237060546875\n",
      "Loss: 5.1821441650390625\n",
      "Loss: 5.115865230560303\n",
      "Loss: 4.952822208404541\n",
      "Loss: 4.764490604400635\n",
      "Loss: 4.647696018218994\n",
      "Loss: 4.699833393096924\n",
      "Loss: 4.925076961517334\n",
      "Loss: 5.214734077453613\n",
      "Loss: 5.483250141143799\n",
      "Loss: 5.596367359161377\n",
      "Loss: 5.458627700805664\n",
      "Loss: 5.078653812408447\n",
      "Loss: 4.607181549072266\n",
      "Loss: 4.2741289138793945\n",
      "Loss: 4.2151899337768555\n",
      "Loss: 4.386983871459961\n",
      "Loss: 4.560690879821777\n",
      "Loss: 4.584988117218018\n",
      "Loss: 4.434788227081299\n",
      "Loss: 4.197868347167969\n",
      "Loss: 4.014474868774414\n",
      "Loss: 3.9462976455688477\n",
      "Loss: 3.9814367294311523\n",
      "Loss: 4.067985534667969\n",
      "Loss: 4.127209663391113\n",
      "Loss: 4.1155500411987305\n",
      "Loss: 4.048784255981445\n",
      "Loss: 3.9176900386810303\n",
      "Loss: 3.761004686355591\n",
      "Loss: 3.680262565612793\n",
      "Loss: 3.6644794940948486\n",
      "Loss: 3.6878528594970703\n",
      "Loss: 3.7095885276794434\n",
      "Loss: 3.7341885566711426\n",
      "Loss: 3.719287633895874\n",
      "Loss: 3.6797192096710205\n",
      "Loss: 3.631662130355835\n",
      "Loss: 3.5832266807556152\n",
      "Loss: 3.5344371795654297\n",
      "Loss: 3.484909772872925\n",
      "Loss: 3.4515457153320312\n",
      "Loss: 3.4305338859558105\n",
      "Loss: 3.4307210445404053\n",
      "Loss: 3.461074113845825\n",
      "Loss: 3.518817663192749\n",
      "Loss: 3.631650447845459\n",
      "Loss: 3.787875175476074\n",
      "Loss: 4.043996334075928\n",
      "Loss: 4.34751558303833\n",
      "Loss: 4.7852044105529785\n",
      "Loss: 5.166317939758301\n",
      "Loss: 5.612653732299805\n",
      "Loss: 5.906314373016357\n",
      "Loss: 6.187865734100342\n",
      "Loss: 6.086124420166016\n",
      "Loss: 5.631648540496826\n",
      "Loss: 4.699457168579102\n",
      "Loss: 3.7455978393554688\n",
      "Loss: 3.268259286880493\n",
      "Loss: 3.411609649658203\n",
      "Loss: 3.8090853691101074\n",
      "Loss: 4.089813232421875\n",
      "Loss: 4.085023880004883\n",
      "Loss: 3.917417049407959\n",
      "Loss: 3.7589335441589355\n",
      "Loss: 3.632906198501587\n",
      "Loss: 3.4989001750946045\n",
      "Loss: 3.3370983600616455\n",
      "Loss: 3.174499750137329\n",
      "Loss: 3.0931925773620605\n",
      "Loss: 3.0918917655944824\n",
      "Loss: 3.1391470432281494\n",
      "Loss: 3.163825273513794\n",
      "Loss: 3.1679482460021973\n",
      "Loss: 3.1301889419555664\n",
      "Loss: 3.109755754470825\n",
      "Loss: 3.108024835586548\n",
      "Loss: 3.156977415084839\n",
      "Loss: 3.1748545169830322\n",
      "Loss: 3.173013925552368\n",
      "Loss: 3.126690149307251\n",
      "Loss: 3.059678792953491\n",
      "Loss: 2.9872119426727295\n",
      "Loss: 2.9524266719818115\n",
      "Loss: 2.925741672515869\n",
      "Loss: 2.8892288208007812\n",
      "Loss: 2.8356287479400635\n",
      "Loss: 2.778914451599121\n",
      "Loss: 2.7450520992279053\n",
      "Loss: 2.7393267154693604\n",
      "Loss: 2.763571262359619\n",
      "Loss: 2.825180768966675\n",
      "Loss: 2.896759510040283\n",
      "Loss: 3.0264816284179688\n",
      "Loss: 3.217118740081787\n",
      "Loss: 3.533580780029297\n",
      "Loss: 4.008622169494629\n",
      "Loss: 4.790575981140137\n",
      "Loss: 5.8295369148254395\n",
      "Loss: 7.2751922607421875\n",
      "Loss: 8.691723823547363\n",
      "Loss: 10.222824096679688\n",
      "Loss: 10.742731094360352\n",
      "Loss: 10.885381698608398\n",
      "Loss: 9.801545143127441\n",
      "Loss: 9.125959396362305\n",
      "Loss: 8.554139137268066\n",
      "Loss: 9.825453758239746\n",
      "Loss: 10.372117042541504\n",
      "Loss: 9.190604209899902\n",
      "Loss: 6.741837501525879\n",
      "Loss: 5.1302809715271\n",
      "Loss: 6.133868217468262\n",
      "Loss: 7.802391052246094\n",
      "Loss: 7.911445140838623\n",
      "Loss: 5.384312629699707\n",
      "Loss: 2.913529872894287\n",
      "Loss: 2.679656744003296\n",
      "Loss: 4.095235824584961\n",
      "Loss: 5.024055480957031\n",
      "Loss: 4.660909175872803\n",
      "Loss: 4.048719882965088\n",
      "Loss: 3.834446668624878\n",
      "Loss: 3.9353466033935547\n",
      "Loss: 3.6834452152252197\n",
      "Loss: 3.2813727855682373\n",
      "Loss: 2.961312770843506\n",
      "Loss: 2.7033329010009766\n",
      "Loss: 2.496068239212036\n",
      "Loss: 2.465766429901123\n",
      "Loss: 2.7094454765319824\n",
      "Loss: 2.9912304878234863\n",
      "Loss: 2.950726270675659\n",
      "Loss: 2.7467033863067627\n",
      "Loss: 2.7495837211608887\n",
      "Loss: 3.02073073387146\n",
      "Loss: 3.290252923965454\n",
      "Loss: 3.20832896232605\n",
      "Loss: 3.0370938777923584\n",
      "Loss: 3.02730131149292\n",
      "Loss: 3.2782468795776367\n",
      "Loss: 3.557903528213501\n",
      "Loss: 3.778319835662842\n",
      "Loss: 3.97029185295105\n",
      "Loss: 4.4017252922058105\n",
      "Loss: 5.042220115661621\n",
      "Loss: 6.081703186035156\n",
      "Loss: 7.277377128601074\n",
      "Loss: 9.199199676513672\n",
      "Loss: 11.362163543701172\n",
      "Loss: 14.822174072265625\n",
      "Loss: 18.16976547241211\n",
      "Loss: 23.182823181152344\n",
      "Loss: 26.21409797668457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 30.54457664489746\n",
      "Loss: 29.934375762939453\n",
      "Loss: 29.66604232788086\n",
      "Loss: 23.867111206054688\n",
      "Loss: 18.490726470947266\n",
      "Loss: 11.4338960647583\n",
      "Loss: 6.199906349182129\n",
      "Loss: 2.844707489013672\n",
      "Loss: 1.8587191104888916\n",
      "Loss: 2.7932558059692383\n",
      "Loss: 4.886397838592529\n",
      "Loss: 7.475781440734863\n",
      "Loss: 9.491484642028809\n",
      "Loss: 11.181182861328125\n",
      "Loss: 11.351943969726562\n",
      "Loss: 11.185943603515625\n",
      "Loss: 9.731141090393066\n",
      "Loss: 8.301527976989746\n",
      "Loss: 6.3973388671875\n",
      "Loss: 4.840169429779053\n",
      "Loss: 3.4587414264678955\n",
      "Loss: 2.5133330821990967\n",
      "Loss: 1.9270762205123901\n",
      "Loss: 1.679457426071167\n",
      "Loss: 1.691792368888855\n",
      "Loss: 1.8917213678359985\n",
      "Loss: 2.2274491786956787\n",
      "Loss: 2.650104284286499\n",
      "Loss: 3.1793010234832764\n",
      "Loss: 3.7667236328125\n",
      "Loss: 4.5448317527771\n",
      "Loss: 5.405576705932617\n",
      "Loss: 6.6920166015625\n",
      "Loss: 8.183852195739746\n",
      "Loss: 10.668134689331055\n",
      "Loss: 13.499873161315918\n",
      "Loss: 18.474573135375977\n",
      "Loss: 23.902332305908203\n",
      "Loss: 34.0943717956543\n",
      "Loss: 43.862823486328125\n",
      "Loss: 63.233848571777344\n",
      "Loss: 76.83293914794922\n",
      "Loss: 106.63335418701172\n",
      "Loss: 114.53874969482422\n",
      "Loss: 141.78662109375\n",
      "Loss: 125.1918716430664\n",
      "Loss: 123.58011627197266\n",
      "Loss: 82.41168975830078\n",
      "Loss: 53.362892150878906\n",
      "Loss: 21.191347122192383\n",
      "Loss: 4.793182849884033\n",
      "Loss: 3.326232671737671\n",
      "Loss: 13.259956359863281\n",
      "Loss: 28.526086807250977\n",
      "Loss: 37.520931243896484\n",
      "Loss: 42.960636138916016\n",
      "Loss: 34.13640594482422\n",
      "Loss: 24.086620330810547\n",
      "Loss: 11.385700225830078\n",
      "Loss: 3.7842276096343994\n",
      "Loss: 2.119844436645508\n",
      "Loss: 5.524349212646484\n",
      "Loss: 11.381354331970215\n",
      "Loss: 15.678674697875977\n",
      "Loss: 18.1772518157959\n",
      "Loss: 15.853718757629395\n",
      "Loss: 12.303552627563477\n",
      "Loss: 7.286346912384033\n",
      "Loss: 3.5854854583740234\n",
      "Loss: 1.7100260257720947\n",
      "Loss: 1.8658697605133057\n",
      "Loss: 3.459678888320923\n",
      "Loss: 5.515446662902832\n",
      "Loss: 7.455533981323242\n",
      "Loss: 8.242668151855469\n",
      "Loss: 8.429546356201172\n",
      "Loss: 7.359895706176758\n",
      "Loss: 6.098991870880127\n",
      "Loss: 4.470154762268066\n",
      "Loss: 3.144312858581543\n",
      "Loss: 2.1220834255218506\n",
      "Loss: 1.5630940198898315\n",
      "Loss: 1.4126535654067993\n",
      "Loss: 1.5860698223114014\n",
      "Loss: 1.984073281288147\n",
      "Loss: 2.4874777793884277\n",
      "Loss: 3.0730972290039062\n",
      "Loss: 3.606282949447632\n",
      "Loss: 4.217671871185303\n",
      "Loss: 4.720248222351074\n",
      "Loss: 5.411593437194824\n",
      "Loss: 6.018651008605957\n",
      "Loss: 7.0411481857299805\n",
      "Loss: 7.993830680847168\n",
      "Loss: 9.736204147338867\n",
      "Loss: 11.442008018493652\n",
      "Loss: 14.531890869140625\n",
      "Loss: 17.046085357666016\n",
      "Loss: 21.591815948486328\n",
      "Loss: 24.42420196533203\n",
      "Loss: 30.632841110229492\n",
      "Loss: 34.9780387878418\n",
      "Loss: 47.300594329833984\n",
      "Loss: 58.739994049072266\n",
      "Loss: 87.90996551513672\n",
      "Loss: 109.97850036621094\n",
      "Loss: 167.31439208984375\n",
      "Loss: 190.13241577148438\n",
      "Loss: 273.1726379394531\n",
      "Loss: 257.101318359375\n",
      "Loss: 310.60504150390625\n",
      "Loss: 223.36663818359375\n",
      "Loss: 185.9168243408203\n",
      "Loss: 87.8585433959961\n",
      "Loss: 30.525312423706055\n",
      "Loss: 3.8474035263061523\n",
      "Loss: 13.920652389526367\n",
      "Loss: 47.1546630859375\n",
      "Loss: 73.34420776367188\n",
      "Loss: 95.72965240478516\n",
      "Loss: 74.84312438964844\n",
      "Loss: 51.811859130859375\n",
      "Loss: 19.411788940429688\n",
      "Loss: 3.2369937896728516\n",
      "Loss: 4.930174350738525\n",
      "Loss: 18.319847106933594\n",
      "Loss: 34.55112075805664\n",
      "Loss: 38.3590087890625\n",
      "Loss: 35.92546844482422\n",
      "Loss: 21.384437561035156\n",
      "Loss: 8.927145957946777\n",
      "Loss: 1.884480357170105\n",
      "Loss: 2.7966160774230957\n",
      "Loss: 8.980161666870117\n",
      "Loss: 15.090700149536133\n",
      "Loss: 19.08358383178711\n",
      "Loss: 16.757722854614258\n",
      "Loss: 12.386155128479004\n",
      "Loss: 6.338710308074951\n",
      "Loss: 2.4019477367401123\n",
      "Loss: 1.3713535070419312\n",
      "Loss: 2.845463991165161\n",
      "Loss: 5.5329155921936035\n",
      "Loss: 7.659790992736816\n",
      "Loss: 8.904839515686035\n",
      "Loss: 8.13994312286377\n",
      "Loss: 6.637286186218262\n",
      "Loss: 4.393551826477051\n",
      "Loss: 2.587857723236084\n",
      "Loss: 1.474647879600525\n",
      "Loss: 1.2133612632751465\n",
      "Loss: 1.63119637966156\n",
      "Loss: 2.4274089336395264\n",
      "Loss: 3.3321187496185303\n",
      "Loss: 3.9543254375457764\n",
      "Loss: 4.326416015625\n",
      "Loss: 4.171091556549072\n",
      "Loss: 3.8289754390716553\n",
      "Loss: 3.1908905506134033\n",
      "Loss: 2.5704727172851562\n",
      "Loss: 1.9480645656585693\n",
      "Loss: 1.4780395030975342\n",
      "Loss: 1.1570031642913818\n",
      "Loss: 1.0004109144210815\n",
      "Loss: 0.9645633697509766\n",
      "Loss: 1.0180379152297974\n",
      "Loss: 1.1270618438720703\n",
      "Loss: 1.2699980735778809\n",
      "Loss: 1.4343217611312866\n",
      "Loss: 1.600186824798584\n",
      "Loss: 1.7950456142425537\n",
      "Loss: 1.9928326606750488\n",
      "Loss: 2.2587342262268066\n",
      "Loss: 2.551257848739624\n",
      "Loss: 2.981245994567871\n",
      "Loss: 3.4754159450531006\n",
      "Loss: 4.254395961761475\n",
      "Loss: 5.175328254699707\n",
      "Loss: 6.721700668334961\n",
      "Loss: 8.593263626098633\n",
      "Loss: 11.902586936950684\n",
      "Loss: 15.855899810791016\n",
      "Loss: 23.343687057495117\n",
      "Loss: 31.97353172302246\n",
      "Loss: 49.56549835205078\n",
      "Loss: 67.37714385986328\n",
      "Loss: 108.32073211669922\n",
      "Loss: 140.81686401367188\n",
      "Loss: 229.92593383789062\n",
      "Loss: 267.0062255859375\n",
      "Loss: 416.7054748535156\n",
      "Loss: 387.7760009765625\n",
      "Loss: 504.2829284667969\n",
      "Loss: 336.3602294921875\n",
      "Loss: 273.0545349121094\n",
      "Loss: 108.71705627441406\n",
      "Loss: 23.522775650024414\n",
      "Loss: 2.376828908920288\n",
      "Loss: 39.60189437866211\n",
      "Loss: 108.77337646484375\n",
      "Loss: 137.34884643554688\n",
      "Loss: 160.15318298339844\n",
      "Loss: 99.35079956054688\n",
      "Loss: 49.251731872558594\n",
      "Loss: 8.620902061462402\n",
      "Loss: 4.119978427886963\n",
      "Loss: 27.817760467529297\n",
      "Loss: 53.525352478027344\n",
      "Loss: 72.88865661621094\n",
      "Loss: 59.900455474853516\n",
      "Loss: 46.18522644042969\n",
      "Loss: 22.680273056030273\n",
      "Loss: 10.705801963806152\n",
      "Loss: 12.779760360717773\n",
      "Loss: 24.227510452270508\n",
      "Loss: 37.59233856201172\n",
      "Loss: 36.03114700317383\n",
      "Loss: 29.288349151611328\n",
      "Loss: 15.335651397705078\n",
      "Loss: 6.7479095458984375\n",
      "Loss: 4.498364448547363\n",
      "Loss: 7.521826267242432\n",
      "Loss: 12.442235946655273\n",
      "Loss: 14.899059295654297\n",
      "Loss: 16.000411987304688\n",
      "Loss: 13.127633094787598\n",
      "Loss: 9.40539836883545\n",
      "Loss: 4.642481327056885\n",
      "Loss: 1.9237658977508545\n",
      "Loss: 2.1842195987701416\n",
      "Loss: 4.755482196807861\n",
      "Loss: 7.715004920959473\n",
      "Loss: 8.440498352050781\n",
      "Loss: 7.521706581115723\n",
      "Loss: 5.075628280639648\n",
      "Loss: 3.18801212310791\n",
      "Loss: 2.0960912704467773\n",
      "Loss: 1.7457799911499023\n",
      "Loss: 1.743377447128296\n",
      "Loss: 2.043174982070923\n",
      "Loss: 2.7077341079711914\n",
      "Loss: 3.423682451248169\n",
      "Loss: 3.900538444519043\n",
      "Loss: 3.5970935821533203\n",
      "Loss: 2.8793041706085205\n",
      "Loss: 1.9746687412261963\n",
      "Loss: 1.3491531610488892\n",
      "Loss: 1.0294510126113892\n",
      "Loss: 0.9113484621047974\n",
      "Loss: 0.8756970167160034\n",
      "Loss: 0.9455959796905518\n",
      "Loss: 1.1661196947097778\n",
      "Loss: 1.4762955904006958\n",
      "Loss: 1.7697120904922485\n",
      "Loss: 1.884501338005066\n",
      "Loss: 1.8739689588546753\n",
      "Loss: 1.738768458366394\n",
      "Loss: 1.634455919265747\n",
      "Loss: 1.5271977186203003\n",
      "Loss: 1.4429600238800049\n",
      "Loss: 1.3108657598495483\n",
      "Loss: 1.1698557138442993\n",
      "Loss: 1.0268863439559937\n",
      "Loss: 0.9352964162826538\n",
      "Loss: 0.885877251625061\n",
      "Loss: 0.8551464080810547\n",
      "Loss: 0.8200855255126953\n",
      "Loss: 0.7732929587364197\n",
      "Loss: 0.7283027172088623\n",
      "Loss: 0.7074907422065735\n",
      "Loss: 0.7126386761665344\n",
      "Loss: 0.7428168058395386\n",
      "Loss: 0.782589316368103\n",
      "Loss: 0.8327434659004211\n",
      "Loss: 0.8987488746643066\n",
      "Loss: 1.0096282958984375\n",
      "Loss: 1.1829962730407715\n",
      "Loss: 1.4731528759002686\n",
      "Loss: 1.8949941396713257\n",
      "Loss: 2.594656229019165\n",
      "Loss: 3.6126749515533447\n",
      "Loss: 5.394240379333496\n",
      "Loss: 7.993183612823486\n",
      "Loss: 12.81915283203125\n",
      "Loss: 19.64021873474121\n",
      "Loss: 33.27988815307617\n",
      "Loss: 51.044700622558594\n",
      "Loss: 90.7302474975586\n",
      "Loss: 133.88182067871094\n",
      "Loss: 246.49658203125\n",
      "Loss: 322.5755310058594\n",
      "Loss: 584.810302734375\n",
      "Loss: 579.7871704101562\n",
      "Loss: 875.5352172851562\n",
      "Loss: 551.5723266601562\n",
      "Loss: 461.9010314941406\n",
      "Loss: 163.99822998046875\n",
      "Loss: 31.840015411376953\n",
      "Loss: 5.946319103240967\n",
      "Loss: 68.136962890625\n",
      "Loss: 176.94686889648438\n",
      "Loss: 194.42828369140625\n",
      "Loss: 201.04208374023438\n",
      "Loss: 94.6537857055664\n",
      "Loss: 23.92228126525879\n",
      "Loss: 1.801283597946167\n",
      "Loss: 32.34440231323242\n",
      "Loss: 84.66696166992188\n",
      "Loss: 93.769775390625\n",
      "Loss: 83.24404907226562\n",
      "Loss: 34.763858795166016\n",
      "Loss: 5.183984756469727\n",
      "Loss: 3.9506795406341553\n",
      "Loss: 27.3996524810791\n",
      "Loss: 82.23631286621094\n",
      "Loss: 120.21935272216797\n",
      "Loss: 157.60934448242188\n",
      "Loss: 102.1183090209961\n",
      "Loss: 44.67546844482422\n",
      "Loss: 3.702554941177368\n",
      "Loss: 11.769938468933105\n",
      "Loss: 50.0714225769043\n",
      "Loss: 68.76348876953125\n",
      "Loss: 67.13497924804688\n",
      "Loss: 30.49428367614746\n",
      "Loss: 4.910933017730713\n",
      "Loss: 2.9456725120544434\n",
      "Loss: 19.6824893951416\n",
      "Loss: 38.16658401489258\n",
      "Loss: 35.609439849853516\n",
      "Loss: 23.16117286682129\n",
      "Loss: 6.271462917327881\n",
      "Loss: 0.624858558177948\n",
      "Loss: 6.923912525177002\n",
      "Loss: 16.34646987915039\n",
      "Loss: 21.523632049560547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15.863903045654297\n",
      "Loss: 7.532896995544434\n",
      "Loss: 1.3409781455993652\n",
      "Loss: 1.4557793140411377\n",
      "Loss: 6.03462028503418\n",
      "Loss: 10.01872444152832\n",
      "Loss: 10.9480562210083\n",
      "Loss: 7.335797309875488\n",
      "Loss: 3.1631767749786377\n",
      "Loss: 0.7526749968528748\n",
      "Loss: 1.328403353691101\n",
      "Loss: 3.73933482170105\n",
      "Loss: 5.711449146270752\n",
      "Loss: 6.252884387969971\n",
      "Loss: 4.786104202270508\n",
      "Loss: 3.0498926639556885\n",
      "Loss: 2.0274548530578613\n",
      "Loss: 2.498608350753784\n",
      "Loss: 4.228428840637207\n",
      "Loss: 6.175776958465576\n",
      "Loss: 8.068861961364746\n",
      "Loss: 8.827656745910645\n",
      "Loss: 9.439692497253418\n",
      "Loss: 9.432143211364746\n",
      "Loss: 9.989227294921875\n",
      "Loss: 10.264566421508789\n",
      "Loss: 9.8168363571167\n",
      "Loss: 8.940685272216797\n",
      "Loss: 6.808638572692871\n",
      "Loss: 4.43529748916626\n",
      "Loss: 2.214698076248169\n",
      "Loss: 1.154005765914917\n",
      "Loss: 1.282561182975769\n",
      "Loss: 2.187896251678467\n",
      "Loss: 3.2882096767425537\n",
      "Loss: 3.9544425010681152\n",
      "Loss: 4.043644428253174\n",
      "Loss: 3.3659911155700684\n",
      "Loss: 2.474311351776123\n",
      "Loss: 1.5420160293579102\n",
      "Loss: 0.9998259544372559\n",
      "Loss: 0.8351468443870544\n",
      "Loss: 0.9667564630508423\n",
      "Loss: 1.1963253021240234\n",
      "Loss: 1.4158389568328857\n",
      "Loss: 1.5151199102401733\n",
      "Loss: 1.4666827917099\n",
      "Loss: 1.3137363195419312\n",
      "Loss: 1.0745443105697632\n",
      "Loss: 0.8856478929519653\n",
      "Loss: 0.7780474424362183\n",
      "Loss: 0.7728206515312195\n",
      "Loss: 0.8055324554443359\n",
      "Loss: 0.8519958257675171\n",
      "Loss: 0.836699366569519\n",
      "Loss: 0.7772100567817688\n",
      "Loss: 0.6588776707649231\n",
      "Loss: 0.5227462649345398\n",
      "Loss: 0.40703654289245605\n",
      "Loss: 0.33949393033981323\n",
      "Loss: 0.3256329298019409\n",
      "Loss: 0.3529614210128784\n",
      "Loss: 0.40027526021003723\n",
      "Loss: 0.4511416256427765\n",
      "Loss: 0.5024513006210327\n",
      "Loss: 0.5433428883552551\n",
      "Loss: 0.5817692875862122\n",
      "Loss: 0.6074377298355103\n",
      "Loss: 0.6382818818092346\n",
      "Loss: 0.6671268343925476\n",
      "Loss: 0.7174146771430969\n",
      "Loss: 0.7807084321975708\n",
      "Loss: 0.8830916285514832\n",
      "Loss: 1.0155467987060547\n",
      "Loss: 1.2285492420196533\n",
      "Loss: 1.5092252492904663\n",
      "Loss: 1.9545857906341553\n",
      "Loss: 2.539733648300171\n",
      "Loss: 3.4956107139587402\n",
      "Loss: 4.756049633026123\n",
      "Loss: 6.906541347503662\n",
      "Loss: 9.716108322143555\n",
      "Loss: 14.813589096069336\n",
      "Loss: 21.190956115722656\n",
      "Loss: 33.721920013427734\n",
      "Loss: 48.105567932128906\n",
      "Loss: 79.58428192138672\n",
      "Loss: 109.564208984375\n",
      "Loss: 185.77908325195312\n",
      "Loss: 233.31216430664062\n",
      "Loss: 391.9731140136719\n",
      "Loss: 406.0341796875\n",
      "Loss: 609.7457275390625\n",
      "Loss: 463.1510009765625\n",
      "Loss: 499.3790588378906\n",
      "Loss: 257.2650451660156\n",
      "Loss: 133.89596557617188\n",
      "Loss: 30.63689422607422\n",
      "Loss: 4.43601131439209\n",
      "Loss: 25.26531982421875\n",
      "Loss: 64.91486358642578\n",
      "Loss: 113.39752960205078\n",
      "Loss: 110.22582244873047\n",
      "Loss: 104.7154312133789\n",
      "Loss: 57.466041564941406\n",
      "Loss: 24.196134567260742\n",
      "Loss: 5.133458614349365\n",
      "Loss: 8.040801048278809\n",
      "Loss: 25.316722869873047\n",
      "Loss: 39.91715621948242\n",
      "Loss: 50.48332214355469\n",
      "Loss: 51.938350677490234\n",
      "Loss: 70.18458557128906\n",
      "Loss: 66.61003112792969\n",
      "Loss: 55.50052261352539\n",
      "Loss: 26.85352897644043\n",
      "Loss: 8.21360969543457\n",
      "Loss: 2.8318932056427\n",
      "Loss: 10.063061714172363\n",
      "Loss: 23.251720428466797\n",
      "Loss: 29.97054672241211\n",
      "Loss: 31.895111083984375\n",
      "Loss: 22.140941619873047\n",
      "Loss: 12.299195289611816\n",
      "Loss: 4.109508514404297\n",
      "Loss: 1.9105069637298584\n",
      "Loss: 4.783487319946289\n",
      "Loss: 9.423519134521484\n",
      "Loss: 13.501795768737793\n",
      "Loss: 13.33681583404541\n",
      "Loss: 11.310408592224121\n",
      "Loss: 7.006364822387695\n",
      "Loss: 3.6209049224853516\n",
      "Loss: 1.670965552330017\n",
      "Loss: 1.6196123361587524\n",
      "Loss: 2.837590456008911\n",
      "Loss: 4.270482063293457\n",
      "Loss: 5.443531036376953\n",
      "Loss: 5.48433780670166\n",
      "Loss: 4.9723358154296875\n",
      "Loss: 3.671365261077881\n",
      "Loss: 2.43074107170105\n",
      "Loss: 1.357671856880188\n",
      "Loss: 0.7901832461357117\n",
      "Loss: 0.7273067235946655\n",
      "Loss: 1.0578982830047607\n",
      "Loss: 1.6156272888183594\n",
      "Loss: 2.0925018787384033\n",
      "Loss: 2.426220417022705\n",
      "Loss: 2.388090133666992\n",
      "Loss: 2.1811604499816895\n",
      "Loss: 1.7460260391235352\n",
      "Loss: 1.3422951698303223\n",
      "Loss: 1.022249698638916\n",
      "Loss: 0.9442972540855408\n",
      "Loss: 1.132879376411438\n",
      "Loss: 1.651281476020813\n",
      "Loss: 2.5333147048950195\n",
      "Loss: 3.7927205562591553\n",
      "Loss: 5.611898422241211\n",
      "Loss: 7.802155017852783\n",
      "Loss: 10.636011123657227\n",
      "Loss: 13.171977996826172\n",
      "Loss: 15.187292098999023\n",
      "Loss: 15.238916397094727\n",
      "Loss: 13.527265548706055\n",
      "Loss: 10.2356595993042\n",
      "Loss: 6.943876266479492\n",
      "Loss: 4.732733726501465\n",
      "Loss: 3.8415632247924805\n",
      "Loss: 3.803133487701416\n",
      "Loss: 3.8470590114593506\n",
      "Loss: 3.974323034286499\n",
      "Loss: 4.046140193939209\n",
      "Loss: 4.559366226196289\n",
      "Loss: 4.936396598815918\n",
      "Loss: 5.262855052947998\n",
      "Loss: 4.709736347198486\n",
      "Loss: 4.059228420257568\n",
      "Loss: 3.570080518722534\n",
      "Loss: 4.152212619781494\n",
      "Loss: 5.399480819702148\n",
      "Loss: 7.285795211791992\n",
      "Loss: 8.62157917022705\n",
      "Loss: 10.452903747558594\n",
      "Loss: 12.018762588500977\n",
      "Loss: 15.952167510986328\n",
      "Loss: 20.610261917114258\n",
      "Loss: 30.131513595581055\n",
      "Loss: 39.36915969848633\n",
      "Loss: 58.8825798034668\n",
      "Loss: 75.09085845947266\n",
      "Loss: 115.94923400878906\n",
      "Loss: 142.0477294921875\n",
      "Loss: 222.20596313476562\n",
      "Loss: 245.6182098388672\n",
      "Loss: 369.0571594238281\n",
      "Loss: 341.6092834472656\n",
      "Loss: 444.3918151855469\n",
      "Loss: 330.4842834472656\n",
      "Loss: 345.7715759277344\n",
      "Loss: 202.87448120117188\n",
      "Loss: 126.48595428466797\n",
      "Loss: 38.9942512512207\n",
      "Loss: 2.7620232105255127\n",
      "Loss: 8.289022445678711\n",
      "Loss: 41.519168853759766\n",
      "Loss: 89.7134017944336\n",
      "Loss: 106.5400390625\n",
      "Loss: 123.7424087524414\n",
      "Loss: 86.47855377197266\n",
      "Loss: 56.49118423461914\n",
      "Loss: 19.945358276367188\n",
      "Loss: 2.3869895935058594\n",
      "Loss: 3.330686092376709\n",
      "Loss: 17.20555305480957\n",
      "Loss: 35.99456024169922\n",
      "Loss: 43.79741668701172\n",
      "Loss: 47.077369689941406\n",
      "Loss: 33.279762268066406\n",
      "Loss: 19.948400497436523\n",
      "Loss: 6.792801856994629\n",
      "Loss: 1.0632168054580688\n",
      "Loss: 2.545856237411499\n",
      "Loss: 8.453688621520996\n",
      "Loss: 15.548689842224121\n",
      "Loss: 18.452964782714844\n",
      "Loss: 18.89051628112793\n",
      "Loss: 13.924484252929688\n",
      "Loss: 8.683713912963867\n",
      "Loss: 3.58003306388855\n",
      "Loss: 0.9990787506103516\n",
      "Loss: 1.0288633108139038\n",
      "Loss: 2.9239580631256104\n",
      "Loss: 5.569852828979492\n",
      "Loss: 7.299283027648926\n",
      "Loss: 8.164384841918945\n",
      "Loss: 7.0998992919921875\n",
      "Loss: 5.498997211456299\n",
      "Loss: 3.313412666320801\n",
      "Loss: 1.6361322402954102\n",
      "Loss: 0.6706936359405518\n",
      "Loss: 0.5656105279922485\n",
      "Loss: 1.1423863172531128\n",
      "Loss: 2.0453553199768066\n",
      "Loss: 3.0414555072784424\n",
      "Loss: 3.7019729614257812\n",
      "Loss: 4.1867289543151855\n",
      "Loss: 4.156956195831299\n",
      "Loss: 4.117620468139648\n",
      "Loss: 3.8452742099761963\n",
      "Loss: 3.85595703125\n",
      "Loss: 3.977236747741699\n",
      "Loss: 4.476834774017334\n",
      "Loss: 5.08266544342041\n",
      "Loss: 5.784893989562988\n",
      "Loss: 6.486742973327637\n",
      "Loss: 6.8534064292907715\n",
      "Loss: 7.244161605834961\n",
      "Loss: 7.0154547691345215\n",
      "Loss: 6.894717693328857\n",
      "Loss: 6.491098403930664\n",
      "Loss: 6.419356346130371\n",
      "Loss: 6.117043495178223\n",
      "Loss: 6.1195526123046875\n",
      "Loss: 5.798452377319336\n",
      "Loss: 5.6804962158203125\n",
      "Loss: 5.22581672668457\n",
      "Loss: 4.937796592712402\n",
      "Loss: 4.361834526062012\n",
      "Loss: 3.9598796367645264\n",
      "Loss: 3.4383106231689453\n",
      "Loss: 3.151508092880249\n",
      "Loss: 2.9131741523742676\n",
      "Loss: 2.992579698562622\n",
      "Loss: 3.1680901050567627\n",
      "Loss: 3.6841723918914795\n",
      "Loss: 4.238021373748779\n",
      "Loss: 5.158934116363525\n",
      "Loss: 5.965670108795166\n",
      "Loss: 7.282376289367676\n",
      "Loss: 8.42508602142334\n",
      "Loss: 10.618627548217773\n",
      "Loss: 12.821523666381836\n",
      "Loss: 17.28607177734375\n",
      "Loss: 21.948522567749023\n",
      "Loss: 31.484472274780273\n",
      "Loss: 40.7144775390625\n",
      "Loss: 60.66725158691406\n",
      "Loss: 76.55159759521484\n",
      "Loss: 116.12169647216797\n",
      "Loss: 138.46058654785156\n",
      "Loss: 209.95530700683594\n",
      "Loss: 226.2910919189453\n",
      "Loss: 328.5611572265625\n",
      "Loss: 300.7784423828125\n",
      "Loss: 380.03826904296875\n",
      "Loss: 278.86798095703125\n",
      "Loss: 265.8135681152344\n",
      "Loss: 146.97776794433594\n",
      "Loss: 82.33336639404297\n",
      "Loss: 23.543373107910156\n",
      "Loss: 1.279693365097046\n",
      "Loss: 8.308599472045898\n",
      "Loss: 33.78509521484375\n",
      "Loss: 77.25439453125\n",
      "Loss: 116.17589569091797\n",
      "Loss: 173.51979064941406\n",
      "Loss: 156.903076171875\n",
      "Loss: 166.19740295410156\n",
      "Loss: 106.6436996459961\n",
      "Loss: 63.0966796875\n",
      "Loss: 17.898483276367188\n",
      "Loss: 0.6909212470054626\n",
      "Loss: 8.803292274475098\n",
      "Loss: 30.755403518676758\n"
     ]
    }
   ],
   "source": [
    "X = load_data(100)\n",
    "\n",
    "X_DIM = 12\n",
    "X_LEN = 50\n",
    "N_EX = 1\n",
    "B_SZ = 1\n",
    "\n",
    "# Shrink the data in accordance with above.\n",
    "X = X[0:X_LEN,0:N_EX,0:X_DIM]\n",
    "\n",
    "def reshape_for_cnn(X):\n",
    "    \"\"\"\n",
    "    Reshapes the data from (X_LEN, B_SZ, X_DIM) (e.g. (5000, 10, 12))\n",
    "    to (B_SZ, 1, X_DIM, X_LEN), because we have 1 channel.\n",
    "    \"\"\"\n",
    "    X_LEN, B_SZ, X_DIM = X.shape\n",
    "    X = torch.permute(X, (1, 2, 0))\n",
    "    X = torch.reshape(X, (B_SZ, 1, X_DIM, X_LEN))\n",
    "    return X\n",
    "\n",
    "X = reshape_for_cnn(X)\n",
    "\n",
    "#mps_device = torch.device(\"mps\")\n",
    "#X.to(mps_device)\n",
    "\n",
    "ae = AutoEncoder()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr = 1e-3, weight_decay = 1e-9)\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "for e_i in range(N_EPOCHS):\n",
    "    \n",
    "    # shuffle the indices\n",
    "    idx = np.arange(N_EX)\n",
    "    np.random.shuffle(idx)\n",
    "    \n",
    "    # iterate over batches\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = N_EX // batch_size\n",
    "    for b_i in range(num_batches):\n",
    "        \n",
    "        # Make batch\n",
    "        X_batch = X[idx[b_i * batch_size : (b_i + 1) * batch_size],:,:,:]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        r = ae.forward(X_batch)\n",
    "\n",
    "        # Compute MSE loss\n",
    "        loss = criterion(r, X_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Loss:\", epoch_loss / num_batches / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb2fcb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    o = X[i:i+1, :, :, :]\n",
    "    r = ae.forward(o)\n",
    "    plot_ecg(o[0, 0, :, :], r[0, 0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65568aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(N_EX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4c82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
